---
layout:     post
title:      deeplearning总结
subtitle:   
date:       2019-12-06
author:     enoshx
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - deeplearning
    - daily
    - note
---

# Deeplearning

##深度学习中的正则化
1. dropout
* 权重比例推断规则

** Dropout在极小训练样本时不会很有效(如5000)

##深度学习中的优化

### 学习与纯优化有什么不同
1.
2. 
3. 批量算法与小批量算法
* 目标函数可以分解为训练样本上的求和
* 大样本计算量与收敛速度不成正比
* 训练集存在冗余

#### 1.小批量的大小的影响因素
* 大批量能计算更精确的梯度估计，但回报（优化速度）小于线性的
* 批量的大小有下限，极小批量难以充分利用多核架构
* 内存消耗与批量大小成正比
* 2的幂数作为批量大小可以获得更小的运行时间
* 小批量可能会加入噪声，产生正则化的效果；需要小学习率来保证稳定性；

#### 2.神经网络优化中的挑战
* 病态
在随机梯度下降过程中卡在某个部分，此时即使很小的更新步长也会增加代价函数（牛顿法有利于解决此问题）
* 局部极小值
局部极小值的代价远大于全局最小值，会导致难以优化
排除局部极小值的影响可以画出梯度范数随时间变化因越小越好
* 高原、鞍点和其他平坦区域
鞍点很常见
*未经优化的二阶优化的牛顿法难以逃脱鞍点，但连续时间的梯度下降能逃离*

* 悬崖和梯度爆炸
悬崖：由于几个较大的权重相乘导致的 (易发生循环神经网络中)
*梯度截断(gradient clipping)*

* 长期依赖
由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难

* 非精确梯度

* 局部和全局结构间的弱对应

* 优化的理论限制

#### 3.基本算法
* 随机梯度下降
![SGD](http:/img/SGD.png)

* 动量
为了解决Hessian矩阵的病态条件和随机梯度的方差
![vSGD](http:/img/vSGD.png)

* Nesterov动量
梯度计算在施加当前速度之后
![Nesterov](http:/img/Nesterov.png)

#### 4.参数初始化策略

#### 5.自适应学习率算法
* AdaGrad

* RMSProp
指数加权的移动平均

* Adam(adaptive moments)

#### 6.二阶近似方法
* 牛顿法
牛顿法加正则化策略，使H的特征值尽可能为正的

* 共轭梯度

* BFGS

#### 7.优化策略和元算法
* 批标准化

* 坐标下降（coordinate descent)

* Polyak平均

* 监督预训练

* 设计有助于优化的模型

* 延拓法（continuation method）和课程学习（curriculum learning）

### 九、卷积网络 

